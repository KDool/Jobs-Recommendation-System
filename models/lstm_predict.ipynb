{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vankhaido/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vankhaido/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vankhaido/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense,SpatialDropout1D\n",
    "import contractions\n",
    "import re \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# initializing Stop words libraries\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(desc):\n",
    "    desc = contractions.fix(desc)\n",
    "    desc = re.sub(\"[!@.$\\'\\'':()]\", \"\", desc)\n",
    "    return desc\n",
    "\n",
    "def tokenize_and_tag(desc):\n",
    "    tokens = nltk.word_tokenize(desc.lower())\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(filtered_tokens)\n",
    "    return tagged\n",
    "def extract_POS(tagged):\n",
    "    #pattern 1\n",
    "    grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar1)\n",
    "    tree1 = chunkParser.parse(tagged)\n",
    "\n",
    "    # typical noun phrase pattern appending to be concatted later\n",
    "    g1_chunks = []\n",
    "    for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
    "        g1_chunks.append(subtree)\n",
    "    \n",
    "    #pattern 2\n",
    "    grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar2)\n",
    "    tree2 = chunkParser.parse(tagged)\n",
    "\n",
    "    # variation of a noun phrase pattern to be pickled for later analyses\n",
    "    g2_chunks = []\n",
    "    for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
    "        g2_chunks.append(subtree)\n",
    "        \n",
    "    #pattern 3\n",
    "    grammar3 = (''' VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar3)\n",
    "    tree3 = chunkParser.parse(tagged)\n",
    "\n",
    "    # verb-noun pattern appending to be concatted later\n",
    "    g3_chunks = []\n",
    "    for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
    "        g3_chunks.append(subtree)\n",
    "        \n",
    "        \n",
    "    # pattern 4\n",
    "    # any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
    "    grammar4 = ('''Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar4)\n",
    "    tree4 = chunkParser.parse(tagged)\n",
    "\n",
    "    # common pattern of listing skills appending to be concatted later\n",
    "    g4_chunks = []\n",
    "    for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
    "        g4_chunks.append(subtree)\n",
    "        \n",
    "    return g1_chunks, g2_chunks, g3_chunks, g4_chunks\n",
    "\n",
    "\n",
    "def training_set(chunks):\n",
    "    '''creates a dataframe that easily parsed with the chunks data '''\n",
    "    df = pd.DataFrame(chunks)    \n",
    "    df.fillna('X', inplace = True)\n",
    "    \n",
    "    train = []\n",
    "    for row in df.values:\n",
    "        phrase = ''\n",
    "        for tup in row:\n",
    "            # needs a space at the end for seperation\n",
    "            phrase += tup[0] + ' '\n",
    "        phrase = ''.join(phrase)\n",
    "        # could use padding tages but encoder method will provide during \n",
    "        # tokenizing/embeddings; X can replace paddding for now\n",
    "        train.append( phrase.replace('X', '').strip())\n",
    "\n",
    "    df['phrase'] = train\n",
    "\n",
    "    #returns 50% of each dataframe to be used if you want to improve execution time\n",
    "    # return df.phrase.sample(frac = 0.5)\n",
    "    # Update: only do 50% if running on excel\n",
    "    return df.phrase\n",
    "\n",
    "def strip_commas(df):\n",
    "    '''create new series of individual n-grams'''\n",
    "    grams = []\n",
    "    for sen in df:\n",
    "        sent = sen.split(',')\n",
    "        for word in sent:\n",
    "            grams.append(word)\n",
    "    return pd.Series(grams)\n",
    "\n",
    "def generate_phrases(desc):\n",
    "    tagged = tokenize_and_tag(desc)\n",
    "    g1_chunks, g2_chunks, g3_chunks, g4_chunks = extract_POS(tagged)\n",
    "    c = training_set(g4_chunks)       \n",
    "    separated_chunks4 = strip_commas(c)\n",
    "    phrases = pd.concat([training_set(g1_chunks),\n",
    "                          training_set(g2_chunks), \n",
    "                          training_set(g3_chunks),\n",
    "                          separated_chunks4], \n",
    "                            ignore_index = True )\n",
    "    return phrases\n",
    "\n",
    "\"\"\"Creates corpus from feature column, which is a pandas series\"\"\"\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for phrase in tqdm(df):\n",
    "        words=[word.lower() for word in word_tokenize(phrase) if(word.isalpha()==1)]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "\"\"\"Create padded sequences of equal lenght as input to LSTM\"\"\"\n",
    "def create_padded_inputs(corpus):\n",
    "    MAX_LEN=30\n",
    "    tokenizer_obj=Tokenizer()\n",
    "    tokenizer_obj.fit_on_texts(corpus)\n",
    "    sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "    phrase_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "    return phrase_pad\n",
    "\n",
    "def get_predictions(desc):\n",
    "    #clean\n",
    "    desc = clean(desc)\n",
    "    #load model\n",
    "    model = tf.keras.models.load_model('models/lstm_skill_extractor.h5')\n",
    "    #tokenize and convert to phrases\n",
    "    phrases = generate_phrases(desc)\n",
    "    #preprocess unseen data\n",
    "    corpus=create_corpus(phrases)\n",
    "    corpus_pad = create_padded_inputs(corpus)\n",
    "    #get predicted classes\n",
    "    predictions = (model.predict(corpus_pad) > 0.1).astype('int32')\n",
    "    #return predicted skills as list\n",
    "    out = pd.DataFrame({'Phrase':phrases, 'Class':predictions.ravel()})\n",
    "    skills = out.loc[out['Class'] == 1]\n",
    "    return skills['Phrase'].tolist()\n",
    "\n",
    "#test with predictions on ketter's repo\n",
    "def get_predictions_excel(filename):\n",
    "    \"\"\"description column must be titled Job Desc\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Extracted skills'] = df['Job Description'].apply(lambda x: get_predictions(x))\n",
    "    \n",
    "    return df.to_csv('extracted.csv')\n",
    "\n",
    "    #throw error if column name does not exist or file format is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/lt1xdrls5_98kvw_5_1t783c0000gn/T/ipykernel_90028/2700900369.py:87: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  return pd.Series(grams)\n",
      "100%|██████████| 62/62 [00:00<00:00, 10235.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['requirements',\n",
       " 'years industry experience',\n",
       " 'proven business impact',\n",
       " 'degree quantitative field',\n",
       " 'experience python',\n",
       " 'data visualisation tools',\n",
       " 'relational databases',\n",
       " 'procedures ability manage',\n",
       " 'priorities experience',\n",
       " 'multiple stakeholders',\n",
       " 'inferences data',\n",
       " 'appropriate course actions',\n",
       " 'large data sets numbers',\n",
       " 'knowledge technology products',\n",
       " 'able work team',\n",
       " 'requirements',\n",
       " 'industry experience proven business impact',\n",
       " 'degree quantitative field',\n",
       " 'experience python',\n",
       " 'data',\n",
       " 'visualisation tools',\n",
       " 'relational databases',\n",
       " 'ability manage multiple priorities',\n",
       " 'experience',\n",
       " 'multiple stakeholders',\n",
       " 'ability',\n",
       " 'inferences',\n",
       " 'data',\n",
       " 'appropriate course actions',\n",
       " 'large data',\n",
       " 'sets',\n",
       " 'numbers',\n",
       " 'knowledge technology products',\n",
       " 'self-starter can-do attitude able work team',\n",
       " 'preferred experience python',\n",
       " 'tools',\n",
       " 'working',\n",
       " 'stored procedures ability manage',\n",
       " 'adjust',\n",
       " 'changing priorities experience',\n",
       " 'working',\n",
       " 'make inferences data',\n",
       " 'excellence',\n",
       " 'working',\n",
       " 'working knowledge technology products']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test get predictions\n",
    "desc = \"\"\"\n",
    "Requirements\n",
    "0-2+ years of industry experience with proven business impact; degree in quantitative field\n",
    "preferred.\n",
    "Have experience with Python, R (or other tools for statistical analysis), data visualisation tools is\n",
    "a plus.\n",
    "Proficiency in advance SQL skills working with relational databases to write complex queries and stored procedures.\n",
    "Ability to manage multiple priorities and adjust quickly to changing priorities.\n",
    "Experience working with multiple stakeholders across various divisions.\n",
    "Ability to make inferences from data and devise appropriate course of actions.\n",
    "Excellence written and verbal communication skills in English and Vietnamese.\n",
    "Comfortable working with large data sets and numbers.\n",
    "Good working knowledge of the technology products.\n",
    "Self-starter with can-do attitude.\n",
    "Able to work in a team\"\"\"\n",
    "get_predictions(desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
