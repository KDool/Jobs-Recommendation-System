{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vankhaido/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vankhaido/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vankhaido/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense,SpatialDropout1D\n",
    "import contractions\n",
    "import re \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# initializing Stop words libraries\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(desc):\n",
    "    desc = contractions.fix(desc)\n",
    "    desc = re.sub(\"[!@.$\\'\\'':()]\", \"\", desc)\n",
    "    return desc\n",
    "\n",
    "def tokenize_and_tag(desc):\n",
    "    tokens = nltk.word_tokenize(desc.lower())\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(filtered_tokens)\n",
    "    return tagged\n",
    "def extract_POS(tagged):\n",
    "    #pattern 1\n",
    "    grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar1)\n",
    "    tree1 = chunkParser.parse(tagged)\n",
    "\n",
    "    # typical noun phrase pattern appending to be concatted later\n",
    "    g1_chunks = []\n",
    "    for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
    "        g1_chunks.append(subtree)\n",
    "    \n",
    "    #pattern 2\n",
    "    grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar2)\n",
    "    tree2 = chunkParser.parse(tagged)\n",
    "\n",
    "    # variation of a noun phrase pattern to be pickled for later analyses\n",
    "    g2_chunks = []\n",
    "    for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
    "        g2_chunks.append(subtree)\n",
    "        \n",
    "    #pattern 3\n",
    "    grammar3 = (''' VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar3)\n",
    "    tree3 = chunkParser.parse(tagged)\n",
    "\n",
    "    # verb-noun pattern appending to be concatted later\n",
    "    g3_chunks = []\n",
    "    for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
    "        g3_chunks.append(subtree)\n",
    "        \n",
    "        \n",
    "    # pattern 4\n",
    "    # any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
    "    grammar4 = ('''Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar4)\n",
    "    tree4 = chunkParser.parse(tagged)\n",
    "\n",
    "    # common pattern of listing skills appending to be concatted later\n",
    "    g4_chunks = []\n",
    "    for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
    "        g4_chunks.append(subtree)\n",
    "        \n",
    "    return g1_chunks, g2_chunks, g3_chunks, g4_chunks\n",
    "\n",
    "\n",
    "def training_set(chunks):\n",
    "    '''creates a dataframe that easily parsed with the chunks data '''\n",
    "    df = pd.DataFrame(chunks)    \n",
    "    df.fillna('X', inplace = True)\n",
    "    \n",
    "    train = []\n",
    "    for row in df.values:\n",
    "        phrase = ''\n",
    "        for tup in row:\n",
    "            # needs a space at the end for seperation\n",
    "            phrase += tup[0] + ' '\n",
    "        phrase = ''.join(phrase)\n",
    "        # could use padding tages but encoder method will provide during \n",
    "        # tokenizing/embeddings; X can replace paddding for now\n",
    "        train.append( phrase.replace('X', '').strip())\n",
    "\n",
    "    df['phrase'] = train\n",
    "\n",
    "    #returns 50% of each dataframe to be used if you want to improve execution time\n",
    "    # return df.phrase.sample(frac = 0.5)\n",
    "    # Update: only do 50% if running on excel\n",
    "    return df.phrase\n",
    "\n",
    "def strip_commas(df):\n",
    "    '''create new series of individual n-grams'''\n",
    "    grams = []\n",
    "    for sen in df:\n",
    "        sent = sen.split(',')\n",
    "        for word in sent:\n",
    "            grams.append(word)\n",
    "    return pd.Series(grams)\n",
    "\n",
    "def generate_phrases(desc):\n",
    "    tagged = tokenize_and_tag(desc)\n",
    "    g1_chunks, g2_chunks, g3_chunks, g4_chunks = extract_POS(tagged)\n",
    "    c = training_set(g4_chunks)       \n",
    "    separated_chunks4 = strip_commas(c)\n",
    "    phrases = pd.concat([training_set(g1_chunks),\n",
    "                          training_set(g2_chunks), \n",
    "                          training_set(g3_chunks),\n",
    "                          separated_chunks4], \n",
    "                            ignore_index = True )\n",
    "    return phrases\n",
    "\n",
    "\"\"\"Creates corpus from feature column, which is a pandas series\"\"\"\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for phrase in tqdm(df):\n",
    "        words=[word.lower() for word in word_tokenize(phrase) if(word.isalpha()==1)]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "\"\"\"Create padded sequences of equal lenght as input to LSTM\"\"\"\n",
    "def create_padded_inputs(corpus):\n",
    "    MAX_LEN=30\n",
    "    tokenizer_obj=Tokenizer()\n",
    "    tokenizer_obj.fit_on_texts(corpus)\n",
    "    sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "    phrase_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "    return phrase_pad\n",
    "\n",
    "def get_predictions(desc):\n",
    "    #clean\n",
    "    desc = clean(desc)\n",
    "    #load model\n",
    "    model = tf.keras.models.load_model('models/lstm_skill_extractor.h5')\n",
    "    #tokenize and convert to phrases\n",
    "    phrases = generate_phrases(desc)\n",
    "    #preprocess unseen data\n",
    "    corpus=create_corpus(phrases)\n",
    "    corpus_pad = create_padded_inputs(corpus)\n",
    "    #get predicted classes\n",
    "    predictions = (model.predict(corpus_pad) > 0.1).astype('int32')\n",
    "    #return predicted skills as list\n",
    "    out = pd.DataFrame({'Phrase':phrases, 'Class':predictions.ravel()})\n",
    "    skills = out.loc[out['Class'] == 1]\n",
    "    return skills['Phrase'].tolist()\n",
    "\n",
    "#test with predictions on ketter's repo\n",
    "def get_predictions_excel(filename):\n",
    "    \"\"\"description column must be titled Job Desc\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Extracted skills'] = df['Job Description'].apply(lambda x: get_predictions(x))\n",
    "    \n",
    "    return df.to_csv('extracted.csv')\n",
    "\n",
    "    #throw error if column name does not exist or file format is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 15421.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['powerful helpful ai systems',\n",
       " 'reinforcement learning',\n",
       " 'machine learning',\n",
       " 'remote human guidance create',\n",
       " 'real-world problems',\n",
       " 'environments',\n",
       " 'todays',\n",
       " 'engineering',\n",
       " 'back-end infrastructure support ml efforts',\n",
       " 'complex systems',\n",
       " 'various hardware components',\n",
       " 'balance considerations',\n",
       " 'scalability',\n",
       " 'reliability',\n",
       " 'robustness',\n",
       " 'collaborate researchers',\n",
       " 'engineers',\n",
       " 'experiments',\n",
       " 'various big data tools',\n",
       " 'cloud technologies',\n",
       " 'machine',\n",
       " 'big data technologies',\n",
       " 'problems',\n",
       " 'able tackle',\n",
       " 'problems',\n",
       " 'technical skills',\n",
       " 'reliable production quality software',\n",
       " 'time resource constraints',\n",
       " 'complete familiarity',\n",
       " 'modern software development',\n",
       " 'design documentation',\n",
       " 'code reviews',\n",
       " 'ci/cd',\n",
       " 'project management workflow',\n",
       " 'communication',\n",
       " 'organization skills bachelors',\n",
       " 'computer science',\n",
       " 'learning packages',\n",
       " 'deep learning',\n",
       " 'powerful helpful ai systems',\n",
       " 'reinforcement learning',\n",
       " 'machine learning',\n",
       " 'real-world problems',\n",
       " 'environments',\n",
       " 'like todays',\n",
       " 'engineering',\n",
       " 'back-end infrastructure support ml efforts',\n",
       " 'various hardware components',\n",
       " 'balance considerations',\n",
       " 'scalability',\n",
       " 'reliability',\n",
       " 'robustness',\n",
       " 'collaborate researchers',\n",
       " 'engineers',\n",
       " 'packages',\n",
       " 'experiments',\n",
       " 'algorithms experience various big data tools',\n",
       " 'cloud technologies',\n",
       " 'machine',\n",
       " 'big data',\n",
       " 'technologies',\n",
       " 'variety machine',\n",
       " 'problems',\n",
       " 'able tackle',\n",
       " 'problems',\n",
       " 'technical skills',\n",
       " 'significant development experience python linux reliable production quality software',\n",
       " 'complete familiarity modern software development',\n",
       " 'design documentation',\n",
       " 'code reviews',\n",
       " 'ci/cd',\n",
       " 'project management workflow',\n",
       " 'communication',\n",
       " 'organization skills',\n",
       " 'bachelors',\n",
       " 'learning packages',\n",
       " 'deep learning',\n",
       " 'kindred',\n",
       " 'build',\n",
       " 'filled',\n",
       " 'solve real-world problems',\n",
       " 'changing environments',\n",
       " 'searching',\n",
       " 'learning engineer',\n",
       " 'learning engineering',\n",
       " 'develop',\n",
       " 'understand',\n",
       " 'ml',\n",
       " 'learning',\n",
       " 'learning problems',\n",
       " 'balancing',\n",
       " 'testing',\n",
       " 'degree computer science',\n",
       " 'learning ',\n",
       " ' machine learning ',\n",
       " '',\n",
       " 'scalability ',\n",
       " ' reliability ',\n",
       " 'researchers ',\n",
       " ' engineers ',\n",
       " 'design documentation ',\n",
       " ' code reviews ',\n",
       " ' ci/cd',\n",
       " '',\n",
       " ' project management workflow ',\n",
       " '',\n",
       " ' communication ',\n",
       " ' organization skills bachelors',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test get predictions\n",
    "desc = \"\"\"At Kindred, we are on a mission to build human-like intelligence in machines.\n",
    "\n",
    " \n",
    "\n",
    "Since 2014, we've been paving the way for a world filled with more powerful and helpful AI systems. We bring together reinforcement learning, machine learning, and remote human guidance to create intelligent robots that solve real-world problems alongside humans in complex, changing environments like today's supply chain.\n",
    "\n",
    " \n",
    "\n",
    "The company is growing rapidly and we are searching for an individual for a highly-impactful role, has great attention to detail, is a proactive self-starter and interacts easily with all levels of a high-performance team.\n",
    "\n",
    "\n",
    "About this role:\n",
    "\n",
    "As an Intermediate to Senior Machine Learning Engineer, you are a member of our Machine Learning Engineering & SW Platform team. You will design and develop back-end infrastructure to support ML efforts & product features. You will be contributing to design and architecture of complex systems that integrate various hardware components. You will bring up and balance considerations for immediate and future needs for scalability, reliability, security, and robustness. You will closely collaborate with researchers, engineers, and product managers in Toronto and other offices. In this role, you will be part of an on-call rotation in order to service our customers across North America.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "What you bring: \n",
    "\n",
    "Familiar with a variety of Machine Learning tools and packages, understand how to properly set up experiments\n",
    "Good understanding of the mathematical and statistical foundations of ML algorithms\n",
    "Experience with various big data tools and cloud technologies that enable training algorithms at scale\n",
    "Actively stay on top of applied machine learning and big data technologies\n",
    "Desire to investigate potential algorithmic solutions for a variety of machine learning problems\n",
    "Able to tackle varied problems, balancing a long-term mission with short-term requirements\n",
    "\n",
    "Technical Skills: \n",
    "\n",
    "Significant development experience in Python on Linux\n",
    "Reliable production of quality software within time and resource constraints\n",
    "Complete familiarity with modern software development processes such as design documentation, code reviews, CI/CD, testing, project management workflow, and source control conventions\n",
    "Excellent analytical, problem-solving, communication, and organization skills\n",
    "Bachelor's degree in Computer Science or equivalent experience\n",
    "\n",
    "Bonus Qualifications:\n",
    "\n",
    "Proficiency with Golang\n",
    "Experience with deep learning packages such as Tensorflow or PyTorch\n",
    "Background in robotics, computer vision, and deep learning\"\"\"\n",
    "get_predictions(desc)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
