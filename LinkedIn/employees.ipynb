{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Finish importing packages\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and packages for the project \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "print('- Finish importing packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = '../driver/mac/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Finish initializing a driver\n",
      "- Finish importing the login credentials\n",
      "- Finish keying in email\n",
      "- Finish keying in password\n",
      "- Finish Task 1: Login to Linkedin\n"
     ]
    }
   ],
   "source": [
    "def login():\n",
    "      sleep(2)\n",
    "      url = 'https://www.linkedin.com/login'\n",
    "      driver.get(url)\n",
    "      print('- Finish initializing a driver')\n",
    "      sleep(1)\n",
    "\n",
    "      # Task 1.2: Import username and password\n",
    "      credential = open('credentials.txt')\n",
    "      line = credential.readlines()\n",
    "      username = line[0]\n",
    "      password = line[1]\n",
    "      print('- Finish importing the login credentials')\n",
    "      sleep(1)\n",
    "\n",
    "      # Task 1.2: Key in login credentials\n",
    "      email_field = driver.find_element_by_id('username')\n",
    "      email_field.send_keys(username)\n",
    "      print('- Finish keying in email')\n",
    "      sleep(3)\n",
    "\n",
    "      password_field = driver.find_element_by_name('session_password')\n",
    "      password_field.send_keys(password)\n",
    "      print('- Finish keying in password')\n",
    "      # sleep(3)\n",
    "\n",
    "      # Task 1.2: Click the Login button\n",
    "      # signin_field = driver.find_element_by_xpath('//*[@id=\"organic-div\"]/form/div[3]/button')\n",
    "      # signin_field.click()\n",
    "      # sleep(1.5)\n",
    "\n",
    "      print('- Finish Task 1: Login to Linkedin')\n",
    "\n",
    "      # Task 1.3: Check if the login is successful\n",
    "login()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Finish Task 2: Search for profiles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def search(key=''):\n",
    "    search_field = driver.find_element_by_class_name('search-global-typeahead__input')\n",
    "    # Task 2.2: Input the search query to the search bar\n",
    "    keyword = key + ' ' + 'people'\n",
    "    search_field.send_keys(keyword)\n",
    "\n",
    "    # Task 2.3: Search\n",
    "    search_field.send_keys(Keys.RETURN)\n",
    "    print('- Finish Task 2: Search for profiles')\n",
    "    sleep(2)\n",
    "    see_full_result = driver.find_element_by_css_selector('#main > div > div > div:nth-child(1) > div.search-results__cluster-bottom-banner.artdeco-button.artdeco-button--tertiary.artdeco-button--muted > a')\n",
    "    see_full_result.click()\n",
    "    \n",
    "search('Tester')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Finish Task 3: Scrape the URLs\n",
      "                                           0\n",
      "0                        linh-pham-122506154\n",
      "1    ACoAACjDBdMBE4YEOv_J_QSf1JSo5ya4fXgcnpw\n",
      "2    ACoAADTRcgEBUKQl8zGKUx57LZZu0_-xpCL6Q1M\n",
      "3                                    results\n",
      "4                               hoangnhung97\n",
      "..                                       ...\n",
      "98   ACoAAC_9vS8BbgbsC6_0_dDLUGEITlbOU1VckYI\n",
      "99                                   hieult7\n",
      "100  ACoAAAS8FZEBnFw4Dd9nzwDj5o8acYQUSECwBM4\n",
      "101                uyen-hoang-yuna-8a20761a8\n",
      "102  ACoAABwmrh4BkuWXOARUyHZFwzyMRWAMij6hd6A\n",
      "\n",
      "[103 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Scrape the URLs of the profiles\n",
    "\n",
    "# Task 3.1: Write a function to extract the URLs of one page\n",
    "def GetURL():\n",
    "    page_source = BeautifulSoup(driver.page_source)\n",
    "    profiles = page_source.find_all('a', class_ = 'app-aware-link')\n",
    "    all_profile_URL = []\n",
    "    for profile in profiles:\n",
    "        profile_ID = profile.get('href')\n",
    "        profile_URL = profile_ID.split('?')[0].split('/')[4]\n",
    "        if profile_URL not in all_profile_URL:\n",
    "            all_profile_URL.append(profile_URL)\n",
    "    return all_profile_URL\n",
    "\n",
    "# Task 3.2: Navigate through many page, and extract the profile URLs of each page\n",
    "\n",
    "def FullURL():\n",
    "    input_page = 4\n",
    "    URLs_all_page = []\n",
    "    for page in range(input_page):\n",
    "        URLs_one_page = GetURL()\n",
    "        # print(URLs_one_page)\n",
    "        sleep(2)\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);') #scroll to the end of the page\n",
    "        sleep(3)\n",
    "        # next_button = driver.find_element_by_class_name(\"artdeco-pagination__button--next\")\n",
    "        try:\n",
    "            next_button = driver.find_element_by_css_selector('[aria-label=Next]')\n",
    "        except:\n",
    "            print(\"Error when Loading\")\n",
    "            driver.refresh()\n",
    "            continue\n",
    "            \n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        URLs_all_page = URLs_all_page + URLs_one_page\n",
    "        sleep(3)\n",
    "    return URLs_all_page\n",
    "\n",
    "print('- Finish Task 3: Scrape the URLs')\n",
    "\n",
    "# URLs_all_page = FullURL()\n",
    "\n",
    "\n",
    "# import pandas as pd \n",
    "# df = pd.DataFrame(URLs_all_page)\n",
    "# print(df)\n",
    "# # df.to_csv('profileURL_Tester.csv',mode='a', header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH:  8\n",
      "['Python (Programming Language)', 'Analytical Skills', 'English', 'Engineering']\n"
     ]
    }
   ],
   "source": [
    "def getUserProfile(id=''):\n",
    "    driver.get('https://www.linkedin.com/in/' + id)\n",
    "    name = ''\n",
    "    name = driver.find_elements_by_class_name('text-heading-xlarge')[0].text\n",
    "    print(\"Name: \", name)\n",
    "    driver.get('https://www.linkedin.com/in/' + id +'/details/experience')\n",
    "    # experience_list = driver.find_elements_by_class_name('optional-action-target-wrapper')\n",
    "\n",
    "    \n",
    "    for u in experience_list:\n",
    "        print(u.text)\n",
    "    # Task 4: Scrape the profile information\n",
    "    return\n",
    "\n",
    "\n",
    "def getExperience(id=''):\n",
    "    driver.get('https://www.linkedin.com/in/' + id +'/details/experience')\n",
    "    experience = []\n",
    "    page_source =  BeautifulSoup(driver.page_source,'html.parser')\n",
    "    \n",
    "    list_experience = page_source.select('div.display-flex.flex-row.justify-space-between')\n",
    "    print(\"LENGTH: \", len(list_experience))\n",
    "\n",
    "    count = 0\n",
    "    for item in list_experience:\n",
    "        item_hidden = item.find_all('span',{'aria-hidden':'true'})\n",
    "        count += 1\n",
    "        print(\"COUNT: \", count)\n",
    "        exp = ''\n",
    "        for i in item_hidden:\n",
    "            exp = exp + i.text + ','\n",
    "        exp = exp[:-1]\n",
    "        experience.append(exp)\n",
    "\n",
    "    return experience\n",
    "\n",
    "def getEducation(id=''):\n",
    "    driver.get('https://www.linkedin.com/in/' + id +'/details/education')\n",
    "    education = []\n",
    "    page_source =  BeautifulSoup(driver.page_source,'html.parser')\n",
    "    \n",
    "    list_experience = page_source.select('div.display-flex.flex-row.justify-space-between')\n",
    "    print(\"LENGTH: \", len(list_experience))\n",
    "\n",
    "    count = 0\n",
    "    for item in list_experience:\n",
    "        item_hidden = item.find_all('span',{'aria-hidden':'true'})\n",
    "        count += 1\n",
    "        print(\"COUNT: \", count)\n",
    "        exp = ''\n",
    "        for i in item_hidden:\n",
    "            exp = exp + i.text + ','\n",
    "        exp = exp[:-1]\n",
    "        education.append(exp)\n",
    "    return education\n",
    "\n",
    "\n",
    "def getSkills(id=''):\n",
    "    driver.get('https://www.linkedin.com/in/' + id +'/details/skills')\n",
    "    # pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated \n",
    "    skill_list = []\n",
    "    page_source =  BeautifulSoup(driver.page_source,'html.parser')\n",
    "    list_skill = page_source.select('span.mr1.t-bold')\n",
    "    print(\"LENGTH: \", len(list_skill))\n",
    "    for item in list_skill:\n",
    "        item_list = item.find_all('span',{'aria-hidden':'true'})\n",
    "        for i in item_list:\n",
    "            if i.text not in skill_list:\n",
    "                skill_list.append(i.text)\n",
    "    print(skill_list)\n",
    "\n",
    "\n",
    "# getUserProfile('jolly-nguyen-5a2558179')\n",
    "# getExperience('ân-vũ-944940144')\n",
    "# education = getEducation('thanh-truc-do-30946a235')\n",
    "# education\n",
    "getSkills('thanh-truc-do-30946a235')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mission Completed!\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Scrape the data of 1 Linkedin profile, and write the data to a .CSV file\n",
    "\n",
    "with open('output.csv', 'w',  newline = '') as file_output:\n",
    "    headers = ['Name', 'Job Title', 'Location', 'URL']\n",
    "    writer = csv.DictWriter(file_output, delimiter=',', lineterminator='\\n',fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for linkedin_URL in URLs_all_page:\n",
    "        driver.get(linkedin_URL)\n",
    "        print('- Accessing profile: ', linkedin_URL)\n",
    "\n",
    "        page_source = bs4(driver.page_source, \"html.parser\")\n",
    "\n",
    "        info_div = page_source.find('div',{'class':'flex-1 mr5'})\n",
    "        info_loc = info_div.find_all('ul')\n",
    "        name = info_loc[0].find('li').get_text().strip() #Remove unnecessary characters \n",
    "        print('--- Profile name is: ', name)\n",
    "        location = info_loc[1].find('li').get_text().strip() #Remove unnecessary characters \n",
    "        print('--- Profile location is: ', location)\n",
    "        title = info_div.find('h2').get_text().strip()\n",
    "        print('--- Profile title is: ', title)\n",
    "        writer.writerow({headers[0]:name, headers[1]:location, headers[2]:title, headers[3]:linkedin_URL})\n",
    "        print('\\n')\n",
    "\n",
    "print('Mission Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_URL ='https://www.linkedin.com/in/caroline-almaraz-942476168/'\n",
    "driver.get(linkedin_URL)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "profile_experience = []\n",
    "profile_education = []\n",
    "profile_skills = []\n",
    "profile_certification = []\n",
    "profile_honors = []\n",
    "\n",
    "\n",
    "# profile_name = soup.find_all('h1',{'class':'text-heading-xlarge inline t-24 v-align-middle break-words'})[0].get_text().strip()\n",
    "\n",
    "profile_experience = soup.find_all('section',{'class':'artdeco-card'})\n",
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);') #scroll to the end of the page\n",
    "for i in range(0,len(profile_experience)):\n",
    "      header_tag = profile_experience[i].select('h2.pvs-header__title.text-heading-large>span.visually-hidden')\n",
    "      if len(header_tag) > 0:\n",
    "            if(header_tag[0].get_text().strip() == 'Experience'):\n",
    "                  experience_list = profile_experience[i].select('div.display-flex.flex-row.justify-space-between')\n",
    "                  print(experience_list)           \n",
    "      \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
